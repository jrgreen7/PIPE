{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "christian-sellers",
   "metadata": {},
   "source": [
    "https://medium.com/@fractaldle/guide-to-build-faster-rcnn-in-pytorch-95b10c273439"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "scheduled-cooler",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "image = torch.zeros((1, 3, 800, 800)).float()\n",
    "\n",
    "bbox = torch.FloatTensor([[20, 30, 400, 500], [300, 400, 500, 600]]) # [y1, x1, y2, x2] format\n",
    "labels = torch.LongTensor([6, 8]) # 0 represents background\n",
    "sub_sample = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "auburn-jimmy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "dummy_img = torch.zeros((1, 3, 800, 800)).float()\n",
    "print(dummy_img)\n",
    "#Out: torch.Size([1, 3, 800, 800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cutting-mobility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)]\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.vgg16(pretrained=True)\n",
    "fe = list(model.features)\n",
    "print(fe) # length is 15\n",
    "# [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "#  ReLU(inplace),\n",
    "#  Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "#  ReLU(inplace),\n",
    "#  MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False),\n",
    "#  Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "#  ReLU(inplace),\n",
    "#  Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "#  ReLU(inplace),\n",
    "#  MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False),\n",
    "#  Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "#  ReLU(inplace),\n",
    "#  Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "#  ReLU(inplace),\n",
    "#  Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "#  ReLU(inplace),\n",
    "#  MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False),\n",
    "#  Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "#  ReLU(inplace),\n",
    "#  Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "#  ReLU(inplace),\n",
    "#  Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "#  ReLU(inplace),\n",
    "#  MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False),\n",
    "#  Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "#  ReLU(inplace),\n",
    "#  Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "#  ReLU(inplace),\n",
    "#  Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "#  ReLU(inplace),\n",
    "#  MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "together-cliff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "req_features = []\n",
    "k = dummy_img.clone()\n",
    "for i in fe:\n",
    "    k = i(k)\n",
    "    if k.size()[2] < 800//16:\n",
    "        break\n",
    "    req_features.append(i)\n",
    "    out_channels = k.size()[1]\n",
    "print(len(req_features)) #30\n",
    "print(out_channels) # 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "mineral-effort",
   "metadata": {},
   "outputs": [],
   "source": [
    "faster_rcnn_fe_extractor = nn.Sequential(*req_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "undefined-handle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 50, 50])\n"
     ]
    }
   ],
   "source": [
    "out_map = faster_rcnn_fe_extractor(image)\n",
    "print(out_map.size())\n",
    "\n",
    "# Out: torch.Size([1, 512, 50, 50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-resistance",
   "metadata": {},
   "source": [
    "## Anchor Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "spiritual-unknown",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "ratio = [0.5, 1, 2]\n",
    "anchor_scales = [8, 16, 32]\n",
    "\n",
    "anchor_base = np.zeros((len(ratio) * len(anchor_scales), 4), dtype=np.float32)\n",
    "\n",
    "print(anchor_base)\n",
    "\n",
    "#Out:\n",
    "# array([[0., 0., 0., 0.],\n",
    "#        [0., 0., 0., 0.],\n",
    "#        [0., 0., 0., 0.],\n",
    "#        [0., 0., 0., 0.],\n",
    "#        [0., 0., 0., 0.],\n",
    "#        [0., 0., 0., 0.],\n",
    "#        [0., 0., 0., 0.],\n",
    "#        [0., 0., 0., 0.],\n",
    "#        [0., 0., 0., 0.]], dtype=float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "pacific-slave",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0 8.0\n"
     ]
    }
   ],
   "source": [
    "ctr_y = sub_sample / 2.\n",
    "ctr_x = sub_sample / 2.\n",
    "\n",
    "print(ctr_y, ctr_x)\n",
    "# Out: (8, 8)\n",
    "for i in range(len(ratio)):\n",
    "    for j in range(len(anchor_scales)):\n",
    "        h = sub_sample * anchor_scales[j] * np.sqrt(ratio[i])\n",
    "        w = sub_sample * anchor_scales[j] * np.sqrt(1./ ratio[i])\n",
    "\n",
    "        index = i * len(anchor_scales) + j\n",
    "\n",
    "        anchor_base[index, 0] = ctr_y - h / 2.\n",
    "        anchor_base[index, 1] = ctr_x - w / 2.\n",
    "        anchor_base[index, 2] = ctr_y + h / 2.\n",
    "        anchor_base[index, 3] = ctr_x + w / 2.\n",
    "\n",
    "#Out:\n",
    "# array([[ -37.254833,  -82.50967 ,   53.254833,   98.50967 ],\n",
    "#        [ -82.50967 , -173.01933 ,   98.50967 ,  189.01933 ],\n",
    "#        [-173.01933 , -354.03867 ,  189.01933 ,  370.03867 ],\n",
    "#        [ -56.      ,  -56.      ,   72.      ,   72.      ],\n",
    "#        [-120.      , -120.      ,  136.      ,  136.      ],\n",
    "#        [-248.      , -248.      ,  264.      ,  264.      ],\n",
    "#        [ -82.50967 ,  -37.254833,   98.50967 ,   53.254833],\n",
    "#        [-173.01933 ,  -82.50967 ,  189.01933 ,   98.50967 ],\n",
    "#        [-354.03867 , -173.01933 ,  370.03867 ,  189.01933 ]],\n",
    "#       dtype=float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "floral-manor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -37.254833,  -82.50967 ,   53.254833,   98.50967 ],\n",
       "       [ -82.50967 , -173.01933 ,   98.50967 ,  189.01933 ],\n",
       "       [-173.01933 , -354.03867 ,  189.01933 ,  370.03867 ],\n",
       "       [ -56.      ,  -56.      ,   72.      ,   72.      ],\n",
       "       [-120.      , -120.      ,  136.      ,  136.      ],\n",
       "       [-248.      , -248.      ,  264.      ,  264.      ],\n",
       "       [ -82.50967 ,  -37.254833,   98.50967 ,   53.254833],\n",
       "       [-173.01933 ,  -82.50967 ,  189.01933 ,   98.50967 ],\n",
       "       [-354.03867 , -173.01933 ,  370.03867 ,  189.01933 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuing-brown",
   "metadata": {},
   "source": [
    "# 2. Generate Anchor at all the feature map location."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relevant-sullivan",
   "metadata": {},
   "source": [
    "In-order to do this, we need to first generate the centres for each and every feature map pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "unsigned-steel",
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_size = (800//16)\n",
    "ctr_x = np.arange(16, (fe_size+1) * 16, 16)\n",
    "ctr_y = np.arange(16, (fe_size+1) * 16, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-immigration",
   "metadata": {},
   "source": [
    "Looping through the ctr_x and ctr_y will give us the centers at each and every location. The sudo code is as a below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "civilian-remedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "ctr = np.empty((len(ctr_y)**2, 2))\n",
    "for x in range(len(ctr_x)):\n",
    "    for y in range(len(ctr_y)):\n",
    "        ctr[index, 1] = ctr_x[x] - 8\n",
    "        ctr[index, 0] = ctr_y[y] - 8\n",
    "        index +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cosmetic-leone",
   "metadata": {},
   "source": [
    "The output will be the (x, y) value at each location as shown in the image above. Together we have 2500 anchor centers. Now at each center we need to generate the anchor boxes. This can be done using the code we have used for generating anchor at one location, adding an extract for loop for supplying centers of each anchor will do. Lets see how this is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "accomplished-president",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22500, 4)\n"
     ]
    }
   ],
   "source": [
    "anchors = np.zeros((fe_size * fe_size * 9, 4))\n",
    "index = 0\n",
    "for c in ctr:\n",
    "    ctr_y, ctr_x = c\n",
    "    for i in range(len(ratio)):\n",
    "        for j in range(len(anchor_scales)):\n",
    "            h = sub_sample * anchor_scales[j] * np.sqrt(ratio[i])\n",
    "            w = sub_sample * anchor_scales[j] * np.sqrt(1./ ratio[i])\n",
    "            anchors[index, 0] = ctr_y - h / 2.\n",
    "            anchors[index, 1] = ctr_x - w / 2.\n",
    "            anchors[index, 2] = ctr_y + h / 2.\n",
    "            anchors[index, 3] = ctr_x + w / 2.\n",
    "            index += 1\n",
    "print(anchors.shape)\n",
    "#Out: [22500, 4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "mature-chosen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -37.254834  ,  -82.50966799,   53.254834  ,   98.50966799],\n",
       "       [ -82.50966799, -173.01933598,   98.50966799,  189.01933598],\n",
       "       [-173.01933598, -354.03867197,  189.01933598,  370.03867197],\n",
       "       ...,\n",
       "       [ 701.49033201,  746.745166  ,  882.50966799,  837.254834  ],\n",
       "       [ 610.98066402,  701.49033201,  973.01933598,  882.50966799],\n",
       "       [ 429.96132803,  610.98066402, 1154.03867197,  973.01933598]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plain-boulder",
   "metadata": {},
   "source": [
    "Now since we have generated all the anchor boxes, we need to look at the objects inside the image and assign them to the specific anchor boxes which contain them. Faster_R-CNN has some guidelines to assign labels to the anchor boxes\n",
    "\n",
    "We assign a positive label to two kind of anchors a) The anchor/anchors with the highest Intersection-over-Union(IoU) overlap with a ground-truth-box or b) An anchor that has an IoU overlap higher than 0.7 with ground-truth box.\n",
    "\n",
    "Note that single ground-truth object may assign positive labels to multiple anchors.\n",
    "\n",
    "c) We assign a negative label to a non-positive anchor if its IoU ratio is lower than 0.3 for all ground-truth boxes. d) Anchors that are neither positive nor negitive do not contribute to the training objective.\n",
    "\n",
    "Lets see how this is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dental-internet",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = np.asarray([[20, 30, 400, 500], [300, 400, 500, 600]], dtype=np.float32) # [y1, x1, y2, x2] format\n",
    "labels = np.asarray([6, 8], dtype=np.int8) # 0 represents background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "august-diving",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8940,)\n"
     ]
    }
   ],
   "source": [
    "index_inside = np.where(\n",
    "        (anchors[:, 0] >= 0) &\n",
    "        (anchors[:, 1] >= 0) &\n",
    "        (anchors[:, 2] <= 800) &\n",
    "        (anchors[:, 3] <= 800)\n",
    "    )[0]\n",
    "print(index_inside.shape)\n",
    "#Out: (8940,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "thorough-modem",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1410,  1419,  1428, ..., 21075, 21084, 21093], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_inside"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forbidden-theology",
   "metadata": {},
   "source": [
    "create an empty label array with inside_index shape and fill with -1. Default is set to (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "spiritual-refrigerator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8940,)\n"
     ]
    }
   ],
   "source": [
    "label = np.empty((len(index_inside), ), dtype=np.int32)\n",
    "label.fill(-1)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-silly",
   "metadata": {},
   "source": [
    "create an array with valid anchor boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "occupied-wagner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8940, 4)\n"
     ]
    }
   ],
   "source": [
    "valid_anchor_boxes = anchors[index_inside]\n",
    "print(valid_anchor_boxes.shape)\n",
    "#Out = (8940, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-magnet",
   "metadata": {},
   "source": [
    "For each valid anchor box calculate the iou with each ground truth object. Since we have 8940 anchor boxes and 2 ground truth objects, we should get an array with (8490, 2) as the output. The sudo code for calculating iou between two boxes will be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "running-spencer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 20.  30. 400. 500.]\n",
      " [300. 400. 500. 600.]]\n",
      "(8940, 2)\n"
     ]
    }
   ],
   "source": [
    "ious = np.empty((len(valid_anchor_boxes), 2), dtype=np.float32)\n",
    "ious.fill(0)\n",
    "print(bbox)\n",
    "\n",
    "for num1, i in enumerate(valid_anchor_boxes):\n",
    "    ya1, xa1, ya2, xa2 = i  \n",
    "    anchor_area = (ya2 - ya1) * (xa2 - xa1)\n",
    "    for num2, j in enumerate(bbox):\n",
    "        yb1, xb1, yb2, xb2 = j\n",
    "        box_area = (yb2- yb1) * (xb2 - xb1)\n",
    "        inter_x1 = max([xb1, xa1])\n",
    "        inter_y1 = max([yb1, ya1])\n",
    "        inter_x2 = min([xb2, xa2])\n",
    "        inter_y2 = min([yb2, ya2])\n",
    "        if (inter_x1 < inter_x2) and (inter_y1 < inter_y2):\n",
    "            iter_area = (inter_y2 - inter_y1) * \\\n",
    "(inter_x2 - inter_x1)\n",
    "            iou = iter_area / \\\n",
    "(anchor_area+ box_area - iter_area)            \n",
    "        else:\n",
    "            iou = 0.\n",
    "            \n",
    "        ious[num1, num2] = iou\n",
    "        \n",
    "print(ious.shape)\n",
    "#Out: [22500, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-married",
   "metadata": {},
   "source": [
    "Note: Using numpy arrays, these calculations can be done much more efficiently and with less verbose. However I try to keep here in this way so that people without strong Algebra can also understand.\n",
    "\n",
    "Considering the scenarios of a and b, we need to find two things here\n",
    "\n",
    "    the highest iou for each gt_box and its corresponding anchor box\n",
    "    the highest iou for each anchor box and its corresponding ground truth box"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "associate-stranger",
   "metadata": {},
   "source": [
    "case-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "indie-tutorial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2262 5620]\n",
      "[0.68130493 0.61035156]\n"
     ]
    }
   ],
   "source": [
    "gt_argmax_ious = ious.argmax(axis=0)\n",
    "print(gt_argmax_ious)\n",
    "\n",
    "gt_max_ious = ious[gt_argmax_ious, np.arange(ious.shape[1])]\n",
    "print(gt_max_ious)\n",
    "# Out:\n",
    "# [2262 5620]\n",
    "# [0.68130493 0.61035156]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-programmer",
   "metadata": {},
   "source": [
    "case-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "manual-closer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8940,)\n",
      "[0 0 0 ... 0 0 0]\n",
      "[0.06811669 0.07083762 0.07083762 ... 0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "argmax_ious = ious.argmax(axis=1)\n",
    "print(argmax_ious.shape)\n",
    "print(argmax_ious)\n",
    "max_ious = ious[np.arange(len(index_inside)), argmax_ious]\n",
    "print(max_ious)\n",
    "# Out:\n",
    "# (22500,)\n",
    "# [0, 1, 0, ..., 1, 0, 0]\n",
    "# [0.06811669 0.07083762 0.07083762 ... 0.         0.         0.        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-defendant",
   "metadata": {},
   "source": [
    "Find the anchor_boxes which have this max_ious (gt_max_ious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "challenging-question",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2262 2508 5620 5628 5636 5644 5866 5874 5882 5890 6112 6120 6128 6136\n",
      " 6358 6366 6374 6382]\n"
     ]
    }
   ],
   "source": [
    "gt_argmax_ious = np.where(ious == gt_max_ious)[0]\n",
    "print(gt_argmax_ious)\n",
    "\n",
    "# Out:\n",
    "# [2262, 2508, 5620, 5628, 5636, 5644, 5866, 5874, 5882, 5890, 6112,\n",
    "#        6120, 6128, 6136, 6358, 6366, 6374, 6382]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developed-tennessee",
   "metadata": {},
   "source": [
    "Now we have three arrays\n",
    "\n",
    "    argmax_ious — Tells which ground truth object has max iou with each anchor.\n",
    "    max_ious — Tells the max_iou with ground truth object with each anchor.\n",
    "    gt_argmax_ious — Tells the anchors with the highest Intersection-over-Union (IoU) overlap with a ground-truth box.\n",
    "\n",
    "Using argmax_ious and max_ious we can assign labels and locations to anchor boxes which satisify [b] and [c]. Using gt_argmax_ious we can assign labels and locations to anchor boxes which satisify [a].\n",
    "\n",
    "Lets put thresholds to some variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "pediatric-school",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_iou_threshold = 0.7\n",
    "neg_iou_threshold = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smart-savannah",
   "metadata": {},
   "source": [
    "Assign negitive label (0) to all the anchor boxes which have max_iou less than negitive threshold [c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "sitting-instruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "label[max_ious < neg_iou_threshold] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serial-economics",
   "metadata": {},
   "source": [
    "Assign positive label (1) to all the anchor boxes which have highest IoU overlap with a ground-truth box [a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "million-subject",
   "metadata": {},
   "outputs": [],
   "source": [
    "label[gt_argmax_ious] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-morning",
   "metadata": {},
   "source": [
    "Assign positive label (1) to all the anchor boxes which have max_iou greater than positive threshold [b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "returning-argentina",
   "metadata": {},
   "outputs": [],
   "source": [
    "label[max_ious >= pos_iou_threshold] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "congressional-links",
   "metadata": {},
   "source": [
    "Training RPN The Faster_R-CNN paper phrases as follows Each mini-batch arises from a single image that contains many positive and negitive example anchors, but this will bias towards negitive samples as they are dominate. Instead, we randomly sample 256 anchors in an image to compute the loss function of a mini-batch, where the sampled positive and negative anchors have a ratio of up to 1:1. If there are fewer than 128 positive samples in an image, we pad the mini-batch with negitive ones.. From this we can derive two variable as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "assigned-april",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_ratio = 0.5\n",
    "n_sample = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blessed-ladder",
   "metadata": {},
   "source": [
    "Total positive samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dangerous-canada",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pos = pos_ratio * n_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closed-heading",
   "metadata": {},
   "source": [
    "Now we need to randomly sample n_pos samples from the positive labels and ignore (-1) the remaining ones. In some cases we get less than n_pos samples, in that we will randomly sample (n_sample — n_pos) negitive samples (0) and assign ignore label to the remaining anchor boxes. This is done using the following code.\n",
    "\n",
    "    positive samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "curious-stick",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_index = np.where(label == 1)[0]\n",
    "if len(pos_index) > n_pos:\n",
    "    disable_index = np.random.choice(pos_index, size=(len(pos_index) - n_pos), replace=False)\n",
    "    label[disable_index] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-symbol",
   "metadata": {},
   "source": [
    "negitive samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "identified-cycling",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neg = n_sample * np.sum(label == 1)\n",
    "neg_index = np.where(label == 0)[0]\n",
    "\n",
    "if len(neg_index) > n_neg:\n",
    "    disable_index = np.random.choice(neg_index, size=(len(neg_index) - n_neg), replace = False)\n",
    "    label[disable_index] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moderate-roman",
   "metadata": {},
   "source": [
    "Assigning locations to anchor boxes\n",
    "Now lets assign the locations to each anchor box with the ground truth object which has maximum iou. Note, we will assign anchor locs to all the valid anchor boxes irrespective of its label, later when we are calculating the losses, we can remove them with simple filters.\n",
    "\n",
    "We already know which ground truth object has high iou with each anchor box, Now we need to find the locations of ground truth with respect to the anchor box location. Faster_R-CNN uses the following parametrizion for this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organized-campus",
   "metadata": {},
   "source": [
    "t_{x} = (x - x_{a})/w_{a}\n",
    "\n",
    "t_{y} = (y - y_{a})/h_{a}\n",
    "\n",
    "t_{w} = log(w/ w_a)\n",
    "\n",
    "t_{h} = log(h/ h_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-recall",
   "metadata": {},
   "source": [
    "x, y , w, h are the groud truth box center co-ordinates, width and height. x_a, y_a, h_a and w_a and anchor boxes center cooridinates, width and height."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-escape",
   "metadata": {},
   "source": [
    "For each anchor box, find the groundtruth object which has max_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "rental-lesbian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 20.  30. 400. 500.]\n",
      " [ 20.  30. 400. 500.]\n",
      " [ 20.  30. 400. 500.]\n",
      " ...\n",
      " [ 20.  30. 400. 500.]\n",
      " [ 20.  30. 400. 500.]\n",
      " [ 20.  30. 400. 500.]]\n"
     ]
    }
   ],
   "source": [
    "max_iou_bbox = bbox[argmax_ious]\n",
    "print(max_iou_bbox)#Out\n",
    "# [[ 20.,  30., 400., 500.],\n",
    "#  [ 20.,  30., 400., 500.],\n",
    "#  [ 20.,  30., 400., 500.],\n",
    "#  ...,\n",
    "#  [ 20.,  30., 400., 500.],\n",
    "#  [ 20.,  30., 400., 500.],\n",
    "#  [ 20.,  30., 400., 500.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-state",
   "metadata": {},
   "source": [
    "Inorder to find t_{x}, t_{y}, t_{w}, t_{h}, we need to convert the y1, x1, y2, x2 format of valid anchor boxes and associated ground truth boxes with max iou to ctr_y, ctr_x , h, w format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "unexpected-compensation",
   "metadata": {},
   "outputs": [],
   "source": [
    "height = valid_anchor_boxes[:, 2] - valid_anchor_boxes[:, 0]\n",
    "width = valid_anchor_boxes[:, 3] - valid_anchor_boxes[:, 1]\n",
    "ctr_y = valid_anchor_boxes[:, 0] + 0.5 * height\n",
    "ctr_x = valid_anchor_boxes[:, 1] + 0.5 * width\n",
    "\n",
    "base_height = max_iou_bbox[:, 2] - max_iou_bbox[:, 0]\n",
    "base_width = max_iou_bbox[:, 3] - max_iou_bbox[:, 1]\n",
    "base_ctr_y = max_iou_bbox[:, 0] + 0.5 * base_height\n",
    "base_ctr_x = max_iou_bbox[:, 1] + 0.5 * base_width"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-tradition",
   "metadata": {},
   "source": [
    "Use the above formulas to find the loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "weighted-latitude",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5855728   2.30914558  0.7415674   1.64727602]\n",
      " [ 0.49718446  2.30914558  0.7415674   1.64727602]\n",
      " [ 0.40879611  2.30914558  0.7415674   1.64727602]\n",
      " ...\n",
      " [-2.50801936 -5.29225232  0.7415674   1.64727602]\n",
      " [-2.59640771 -5.29225232  0.7415674   1.64727602]\n",
      " [-2.68479606 -5.29225232  0.7415674   1.64727602]]\n"
     ]
    }
   ],
   "source": [
    "eps = np.finfo(height.dtype).eps\n",
    "height = np.maximum(height, eps)\n",
    "width = np.maximum(width, eps)\n",
    "\n",
    "dy = (base_ctr_y - ctr_y) / height\n",
    "dx = (base_ctr_x - ctr_x) / width\n",
    "dh = np.log(base_height / height)\n",
    "dw = np.log(base_width / width)\n",
    "\n",
    "anchor_locs = np.vstack((dy, dx, dh, dw)).transpose()\n",
    "print(anchor_locs)\n",
    "\n",
    "#Out:\n",
    "# [[ 0.5855727   2.3091455   0.7415673   1.647276  ]\n",
    "#  [ 0.49718437  2.3091455   0.7415673   1.647276  ]\n",
    "#  [ 0.40879607  2.3091455   0.7415673   1.647276  ]\n",
    "#  ...\n",
    "#  [-2.50802    -5.292254    0.7415677   1.6472763 ]\n",
    "#  [-2.5964084  -5.292254    0.7415677   1.6472763 ]\n",
    "#  [-2.6847968  -5.292254    0.7415677   1.6472763 ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatty-choir",
   "metadata": {},
   "source": [
    "    Now we got anchor_locs and label associated with each and every valid anchor boxes\n",
    "\n",
    "Lets map them to the original anchors using the inside_index variable. Fill the unvalid anchor boxes labels with -1 (ignore) and locations with 0.\n",
    "\n",
    "    Final labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "liked-chorus",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_labels = np.empty((len(anchors),), dtype=label.dtype)\n",
    "anchor_labels.fill(-1)\n",
    "anchor_labels[index_inside] = label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naval-camcorder",
   "metadata": {},
   "source": [
    "Final locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "controlling-grove",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_locations = np.empty((len(anchors),) + anchors.shape[1:], dtype=anchor_locs.dtype)\n",
    "anchor_locations.fill(0)\n",
    "anchor_locations[index_inside, :] = anchor_locs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-bulgaria",
   "metadata": {},
   "source": [
    "The final two matrices are\n",
    "\n",
    "    anchor_locations [N, 4] — [22500, 4]\n",
    "    anchor_labels [N,] — [22500]\n",
    "\n",
    "These are used as targets to the RPN network. We will see how this RPN network is designed in the next section.\n",
    "Region Proposal Network.\n",
    "\n",
    "As we have discussed earlier, Prior to this work, region proposals for a network were generated using selective search, CPMC, MCG, Edgeboxes etc. Faster_R-CNN is the first work to demonstrate generating region proposals using deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loose-wellington",
   "metadata": {},
   "source": [
    "## Region Proposal Network.\n",
    "\n",
    "As we have discussed earlier, prior to this work, region proposals for a network were generated using selective search, CPMC, MCG, Edgeboxes etc. Faster_R-CNN is the first work to demonstrate generating region proposals using deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reasonable-person",
   "metadata": {},
   "source": [
    "The network contains a convolution module, on top of which there will be one regression layer, which predicts the location of the box inside the anchor\n",
    "\n",
    "To generate region proposals, we slide a small network over the convolutional feature map output that we obtained in the feature extraction module. This small network takes as input an n x n spatial window of the input convolutional feature map. Each sliding window is mapped to a lower-dimensional feature [512 features]. This feature is fed into two sibling fully connected layers\n",
    "\n",
    "    A box regrression layer\n",
    "    A box classification layer\n",
    "\n",
    "we use n=3, as noted in Faster_R-CNN paper. We can implement this Architecture using n x n convolutional layer followed by two sibiling 1 x 1 convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "blocked-pacific",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "mid_channels = 512\n",
    "in_channels = 512 # depends on the output feature map. in vgg 16 it is equal to 512\n",
    "n_anchor = 9 # Number of anchors at each location\n",
    "conv1 = nn.Conv2d(in_channels, mid_channels, 3, 1, 1)\n",
    "reg_layer = nn.Conv2d(mid_channels, n_anchor *4, 1, 1, 0)\n",
    "cls_layer = nn.Conv2d(mid_channels, n_anchor *2, 1, 1, 0) ## I will be going to use softmax here. you can equally use sigmoid if u replace 2 with 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transsexual-commander",
   "metadata": {},
   "source": [
    "The paper tells that they initialized these layers with zero mean and 0.01 standard deviation for weights and zeros for base. Lets do that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "musical-century",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conv sliding layer\n",
    "conv1.weight.data.normal_(0, 0.01)\n",
    "conv1.bias.data.zero_()# Regression layer\n",
    "reg_layer.weight.data.normal_(0, 0.01)\n",
    "reg_layer.bias.data.zero_()# classification layer\n",
    "cls_layer.weight.data.normal_(0, 0.01)\n",
    "cls_layer.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gentle-defense",
   "metadata": {},
   "source": [
    "Now the outputs we got in the feature extraction state should be sent to this network to predict locations of objects with repect to the anchor and the objectness score assoiciated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "conventional-recorder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 18, 50, 50]) torch.Size([1, 36, 50, 50])\n"
     ]
    }
   ],
   "source": [
    "x = conv1(out_map) # out_map is obtained in section 1\n",
    "pred_anchor_locs = reg_layer(x)\n",
    "pred_cls_scores = cls_layer(x)\n",
    "\n",
    "print(pred_cls_scores.shape, pred_anchor_locs.shape)\n",
    "#Out:\n",
    "#torch.Size([1, 18, 50, 50]) torch.Size([1, 36, 50, 50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-correction",
   "metadata": {},
   "source": [
    "Lets reformat these a bit and make it align with our anchor targets we designed previously. We will also find the objectness scores for each anchor box, as this is used to for proposal layer which we will discuss in the next section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "southern-economics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 22500, 4])\n",
      "torch.Size([1, 50, 50, 18])\n",
      "torch.Size([1, 22500])\n",
      "torch.Size([1, 22500, 2])\n"
     ]
    }
   ],
   "source": [
    "pred_anchor_locs = pred_anchor_locs.permute(0, 2, 3, 1).contiguous().view(1, -1, 4)\n",
    "print(pred_anchor_locs.shape)\n",
    "#Out: torch.Size([1, 22500, 4])\n",
    "\n",
    "pred_cls_scores = pred_cls_scores.permute(0, 2, 3, 1).contiguous()\n",
    "print(pred_cls_scores.shape)\n",
    "#Out torch.Size([1, 50, 50, 18])\n",
    "\n",
    "objectness_score = pred_cls_scores.view(1, 50, 50, 9, 2)[:, :, :, :, 1].contiguous().view(1, -1)\n",
    "print(objectness_score.shape)\n",
    "#Out torch.Size([1, 22500])\n",
    "\n",
    "pred_cls_scores  = pred_cls_scores.view(1, -1, 2)\n",
    "print(pred_cls_scores.shape)\n",
    "# Out torch.size([1, 22500, 2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resistant-mandate",
   "metadata": {},
   "source": [
    "we are done with section\n",
    "\n",
    "    pred_cls_scores and pred_anchor_locs are the output the RPN network and the losses to updates the weights\n",
    "    pred_cls_scores and objectness_scores are used as inputs to the proposal layer, which generate a set of proposal which are further used by RoI network. We will see this in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mobile-lingerie",
   "metadata": {},
   "source": [
    "# Generating proposals to feed Fast R-CNN network\n",
    "\n",
    "The proposal function will take the following parameters\n",
    "\n",
    "    Weather training_mode or testing mode\n",
    "    nms_thresh\n",
    "    n_train_pre_nms — number of bboxes before nms during training\n",
    "    n_train_post_nms — number of bboxes after nms during training\n",
    "    n_test_pre_nms — number of bboxes before nms during testing\n",
    "    n_test_post_nms — number of bboxes after nms during testing\n",
    "    min_size — minimum height of the object required to create a proposal.\n",
    "\n",
    "The Faster R_CNN says, RPN proposals highly overlap with each other. To reduced redundancy, we adopt non-maximum supression (NMS) on the proposal regions based on their cls scores. We fix the IoU threshold for NMS at 0.7, which leaves us about 2000 proposal regions per image. After an ablation study, the authors show that NMS does not harm the ultimate detection accuracy, but substantially reduces the number of proposals. After NMS, we use the top-N ranked proposal regions for detection. In the following we training Fast R-CNN using 2000 RPN proposals. During testing they evaluate only 300 proposals, they have tested this with various numbers and obtained this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "nuclear-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "nms_thresh = 0.7\n",
    "n_train_pre_nms = 12000\n",
    "n_train_post_nms = 2000\n",
    "n_test_pre_nms = 6000\n",
    "n_test_post_nms = 300\n",
    "min_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "durable-fiction",
   "metadata": {},
   "source": [
    "We need to do the following things to generate region of interest proposals to the network.\n",
    "\n",
    "    convert the loc predictions from the rpn network to bbox [y1, x1, y2, x2] format.\n",
    "    clip the predicted boxes to the image\n",
    "    Remove predicted boxes with either height or width < threshold (min_size).\n",
    "    Sort all (proposal, score) pairs by score from highest to lowest.\n",
    "    Take top pre_nms_topN (e.g. 12000 while training and 300 while testing).\n",
    "    Apply nms threshold > 0.7\n",
    "    Take top pos_nms_topN (e.g. 2000 while training and 300 while testing)\n",
    "\n",
    "We will look at each of the stages in the remainder of this section\n",
    "\n",
    "    convert the loc predictions from the rpn network to bbox [y1, x1, y2, x2] format.\n",
    "\n",
    "This is the reverse operations of what we have done while assigning ground truth to anchor boxes .This operation decodes predictions by un-parameterizing them and offseting to image. the formulas are as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "equal-welding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = (w_{a} * ctr_x_{p}) + ctr_x_{a}\n",
    "# y = (h_{a} * ctr_x_{p}) + ctr_x_{a}\n",
    "# h = np.exp(h_{p}) * h_{a}\n",
    "# w = np.exp(w_{p}) * w_{a}and later convert to y1, x1, y2, x2 format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sonic-florence",
   "metadata": {},
   "source": [
    "Convert anchors format from y1, x1, y2, x2 to ctr_x, ctr_y, h, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "executed-therapist",
   "metadata": {},
   "outputs": [],
   "source": [
    "anc_height = anchors[:, 2] - anchors[:, 0]\n",
    "anc_width = anchors[:, 3] - anchors[:, 1]\n",
    "anc_ctr_y = anchors[:, 0] + 0.5 * anc_height\n",
    "anc_ctr_x = anchors[:, 1] + 0.5 * anc_width"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bound-stations",
   "metadata": {},
   "source": [
    "Convert predictions locs using above formulas. before that convert the pred_anchor_locs and objectness_score to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "champion-hardware",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_anchor_locs_numpy = pred_anchor_locs[0].data.numpy()\n",
    "objectness_score_numpy = objectness_score[0].data.numpy()\n",
    "\n",
    "dy = pred_anchor_locs_numpy[:, 0::4] # start:end:step\n",
    "dx = pred_anchor_locs_numpy[:, 1::4]\n",
    "dh = pred_anchor_locs_numpy[:, 2::4]\n",
    "dw = pred_anchor_locs_numpy[:, 3::4]\n",
    "\n",
    "ctr_y = dy * anc_height[:, np.newaxis] + anc_ctr_y[:, np.newaxis]\n",
    "ctr_x = dx * anc_width[:, np.newaxis] + anc_ctr_x[:, np.newaxis]\n",
    "h = np.exp(dh) * anc_height[:, np.newaxis]\n",
    "w = np.exp(dw) * anc_width[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranking-leisure",
   "metadata": {},
   "source": [
    "convert [ctr_x, ctr_y, h, w] to [y1, x1, y2, x2] format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ordered-commerce",
   "metadata": {},
   "outputs": [],
   "source": [
    "roi = np.zeros(pred_anchor_locs_numpy.shape, dtype=pred_anchor_locs_numpy.dtype)\n",
    "roi[:, 0::4] = ctr_y - 0.5 * h\n",
    "roi[:, 1::4] = ctr_x - 0.5 * w\n",
    "roi[:, 2::4] = ctr_y + 0.5 * h\n",
    "roi[:, 3::4] = ctr_x + 0.5 * w\n",
    "\n",
    "#Out:\n",
    "# [[ -36.897102,  -80.29519 ,   54.09939 ,  100.40507 ],\n",
    "#  [ -83.12463 , -165.74298 ,   98.67854 ,  188.6116  ],\n",
    "#  [-170.7821  , -378.22214 ,  196.20844 ,  349.81198 ],\n",
    "#  ...,\n",
    "#  [ 696.17816 ,  747.13306 ,  883.4582  ,  836.77747 ],\n",
    "#  [ 621.42114 ,  703.0614  ,  973.04626 ,  885.31226 ],\n",
    "#  [ 432.86267 ,  622.48926 , 1146.7059  ,  982.9209  ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-ideal",
   "metadata": {},
   "source": [
    "clip the predicted boxes to the image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "informed-excerpt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.         0.        49.285538  99.50046 ]\n",
      " [  0.         0.       103.09419  191.92857 ]\n",
      " [  0.         0.       182.40067  381.00458 ]\n",
      " ...\n",
      " [700.0583   746.72186  800.       800.      ]\n",
      " [615.33276  705.7409   800.       800.      ]\n",
      " [404.3788   612.28644  800.       800.      ]]\n"
     ]
    }
   ],
   "source": [
    "img_size = (800, 800) #Image size\n",
    "roi[:, slice(0, 4, 2)] = np.clip(\n",
    "            roi[:, slice(0, 4, 2)], 0, img_size[0])\n",
    "roi[:, slice(1, 4, 2)] = np.clip(\n",
    "    roi[:, slice(1, 4, 2)], 0, img_size[1])\n",
    "\n",
    "print(roi)\n",
    "\n",
    "#Out:\n",
    "# [[  0.     ,   0.     ,  54.09939, 100.40507],\n",
    "#  [  0.     ,   0.     ,  98.67854, 188.6116 ],\n",
    "#  [  0.     ,   0.     , 196.20844, 349.81198],\n",
    "#  ...,\n",
    "#  [696.17816, 747.13306, 800.     , 800.     ],\n",
    "#  [621.42114, 703.0614 , 800.     , 800.     ],\n",
    "#  [432.86267, 622.48926, 800.     , 800.     ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-final",
   "metadata": {},
   "source": [
    "Remove predicted boxes with either height or width < threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "wired-verse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22500,)\n"
     ]
    }
   ],
   "source": [
    "hs = roi[:, 2] - roi[:, 0]\n",
    "ws = roi[:, 3] - roi[:, 1]\n",
    "keep = np.where((hs >= min_size) & (ws >= min_size))[0]\n",
    "roi = roi[keep, :]\n",
    "score = objectness_score_numpy[keep]\n",
    "\n",
    "print(score.shape)\n",
    "#Out:\n",
    "##(22500, ) all the boxes have minimum size of 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-leader",
   "metadata": {},
   "source": [
    "Sort all (proposal, score) pairs by score from highest to lowest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "precious-firewall",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1317  1326  1308 ... 21610 22483 22060]\n"
     ]
    }
   ],
   "source": [
    "order = score.ravel().argsort()[::-1]\n",
    "print(order)\n",
    "#Out:\n",
    "#[ 889,  929, 1316, ...,  462,  454,    4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternate-observation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
